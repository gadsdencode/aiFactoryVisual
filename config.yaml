project_name: llm-finetuning-visualizer
base_model: mistralai/Mistral-7B-Instruct-v0.3
new_model_name: Llama-2-7b-chat-finetuned
dataset_name: mlabonne/guanaco-llama2-1k
dataset_split: train
text_column: text
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: nf4
  bnb_4bit_compute_dtype: float16
  bnb_4bit_use_double_quant: false
lora:
  r: 64
  alpha: 16
  dropout: 0.1
  bias: none
  task_type: CAUSAL_LM
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  - lm_head
training:
  output_dir: ./results
  num_train_epochs: 1
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  optim: paged_adamw_32bit
  save_steps: 1000
  logging_steps: 1
  learning_rate: 0.0002
  weight_decay: 0.001
  fp16: false
  bf16: false
  max_grad_norm: 0.3
  max_steps: 20
  warmup_ratio: 0.03
  group_by_length: true
  lr_scheduler_type: constant
  report_to: tensorboard
  max_seq_length: null
  packing: false
  device_map: auto
  gradient_checkpointing: false
  attn_implementation: auto
  evaluation_strategy: 'no'
  eval_steps: 100
  tokenizer_num_proc: 8
huggingface:
  push_to_hub: false
  hub_model_id: your-hf-username/your-finetuned-model-name
  hub_token: ''
data_source: local
local_train_path: data/icdu_training_data_v8.jsonl
local_validation_path: data/icdu_validation_data_v8.jsonl
