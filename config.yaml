# ==============================================================================
# Project and Model Configuration
# ==============================================================================

project_name: "llm-finetuning-visualizer"
base_model: "meta-llama/Llama-2-7b-chat-hf" # Base model from Hugging Face Hub
new_model_name: "Llama-2-7b-chat-finetuned" # Name for your fine-tuned model

# ==============================================================================
# Dataset Configuration
# ==============================================================================

dataset_name: "mlabonne/guanaco-llama2-1k" # Dataset from Hugging Face Hub
dataset_split: "train" # e.g., 'train', 'train[:10%]'
text_column: "text" # The column in your dataset that contains the text to be trained on

# ==============================================================================
# Quantization and LoRA Configuration
# ==============================================================================

quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16" # "float16", "bfloat16"
  bnb_4bit_use_double_quant: false

lora:
  r: 64
  alpha: 16
  dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
    - "lm_head"

# ==============================================================================
# Training Arguments Configuration
# For detailed explanations, see Hugging Face's TrainingArguments documentation:
# https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments
# ==============================================================================

training:
  output_dir: "./results"
  num_train_epochs: 1
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 1
  optim: "paged_adamw_32bit"
  save_steps: 25
  logging_steps: 25
  learning_rate: 0.0002
  weight_decay: 0.001
  fp16: false
  bf16: false
  max_grad_norm: 0.3
  max_steps: -1
  warmup_ratio: 0.03
  group_by_length: true
  lr_scheduler_type: "constant"
  report_to: "tensorboard" # or "wandb", "none"
  # SFTTrainer specific arguments
  max_seq_length: null # Set to a specific number, e.g., 512, if needed
  packing: false
  device_map: "auto" # {"": 0} for single GPU, "auto" for multi-GPU

# ==============================================================================
# Hugging Face Hub Integration
# ==============================================================================

huggingface:
  push_to_hub: false
  hub_model_id: "your-hf-username/your-finetuned-model-name" # Replace with your details
  hub_token: "" # Leave empty to use cached token or set explicitly. For security, prefer environment variables.
